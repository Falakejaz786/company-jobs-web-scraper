# ğŸ“ **README â€” Automated Job Discovery & Validation Pipeline**

## ğŸ“Œ Overview

This project automatically identifies company websites, finds their careers pages, extracts job postings, and validates all URLs.
It processes large datasets efficiently using multithreading and outputs a clean, structured Excel file with:

* Company website
* Careers page
* Job titles & URLs
* Link validity checks
* A methodology sheet

This automation drastically reduces manual effort required for job data collection and validation.

---

## ğŸš€ Features

âœ” **Automatic Website Guessing**
Constructs potential company URLs from their names using common TLDs (.com, .org, .net, .io, .co, .ai).

âœ” **Careers Page Detection**
Scans website links for keywords such as `career`, `job`, `join-us`, `work-with-us`.

âœ” **Job Extraction**
Scrapes up to **3 job postings per company** (title + URL).

âœ” **Link Validation**
Checks whether every website, careers page, and job URL is actually reachable.

âœ” **Multithreading for 10Ã— Speed**
Uses `ThreadPoolExecutor` to process many companies in parallel (~2â€“3 minutes instead of 30 minutes).

âœ” **Excel Output (2 Sheets)**

* `Data` â†’ All company/job results
* `Methodology` â†’ Steps followed for data collection

---

## ğŸ“ Project Structure

```
project/
â”‚â”€â”€ script.py
â”‚â”€â”€ Growth For Impact Data Assignment.xlsx
â”‚â”€â”€ companies_jobs_with_methodology_validated.xlsx
â”‚â”€â”€ README.md
```

---

## ğŸ› ï¸ Technologies Used

| Purpose            | Tools / Libraries       |
| ------------------ | ----------------------- |
| Web Scraping       | Requests, BeautifulSoup |
| Data Processing    | Pandas                  |
| Multithreading     | concurrent.futures      |
| File Export        | openpyxl                |
| URL Validation     | HTTP HEAD requests      |
| Logic & Automation | Python 3.8+             |

---

## ğŸ“¥ Input

Upload an Excel file containing at least one column:

```
Company Name
```

Example:

| Company Name      |
| ----------------- |
| Accenture         |
| Growth For Impact |
| Patagonia         |

---

## ğŸ“¤ Output

The script generates:

### `companies_jobs_with_methodology_validated.xlsx`

**Sheet 1 â†’ Data**

* Company Name
* Website + validity
* Careers Page + validity
* Job1 Title, Job1 URL, Job1 Valid
* Job2 Title â€¦
* Job3 Title â€¦

**Sheet 2 â†’ Methodology**
Plain-text documentation of how the data was generated.

---

## ğŸ§  Methodology Summary

1. Normalize company names for URL prediction
2. Generate possible domain names using common TLDs
3. Validate reachable websites
4. Scrape homepage and detect careers-related links
5. Scrape up to 3 jobs from each careers page
6. Validate each job link with HTTP HEAD
7. Run all companies in parallel for faster execution
8. Export structured output + methodology sheet

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install Dependencies

```bash
pip install pandas requests beautifulsoup4 openpyxl
```

### 2ï¸âƒ£ Place your input file in the same folder

```
Growth For Impact Data Assignment (1).xlsx
```

### 3ï¸âƒ£ Run the script

```bash
python script.py
```

### 4ï¸âƒ£ Find your output file

```
companies_jobs_with_methodology_validated.xlsx
```

---

## âš¡ Performance Notes

* ThreadPoolExecutor reduces processing from **~30 min to ~2â€“4 min**
* Timeout of 3 seconds per request keeps the script fast
* Heavy JS-based career pages may not always be detected (no Selenium used)

---

## ğŸ§© Limitations

* Careers pages loaded via JavaScript cannot be scraped fully.
* Only the **first 3 job links** are extracted (customizable).
* URL guessing depends on the company name being recognizable.

---
